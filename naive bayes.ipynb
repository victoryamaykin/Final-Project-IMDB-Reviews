{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import sys\n",
    "from imp import reload\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "if sys.version[0] == '2':\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-726f04c70303>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0municodedata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoktok\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mToktokTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import re,string,unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import datetime\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review\n",
       "0          1  With all this stuff going down at the moment w..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = \"D:/DataRepository/final_project/Final-Project-IMDB-Reviews/labeledTrainData.tsv\"\n",
    "\n",
    "# Import the CSV into a pandas DataFrame\n",
    "df = pd.read_csv(csv_path, low_memory=False, sep = '\\t')\n",
    "df = df.drop(['id'], axis=1)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  With all this stuff going down at the moment w...          1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = df[['review','sentiment']]\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# stop_words = set(stopwords.words(\"english\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
    "    text = text.lower()\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "df['Processed_Reviews'] = df.review.apply(lambda x: clean_text(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Processed_Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>1</td>\n",
       "      <td>stuff go moment mj ive start listen music watc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  With all this stuff going down at the moment w...          1   \n",
       "\n",
       "                                   Processed_Reviews  \n",
       "0  stuff go moment mj ive start listen music watc...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129.54916"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Processed_Reviews.apply(lambda x: len(x.split(\" \"))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 6000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(df['Processed_Reviews'])\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(df['Processed_Reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 130\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aliqu\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\aliqu\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\aliqu\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\aliqu\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - ETA: 7:45 - loss: 0.6933 - acc: 0.440 - ETA: 4:09 - loss: 0.6931 - acc: 0.460 - ETA: 2:57 - loss: 0.6935 - acc: 0.443 - ETA: 2:21 - loss: 0.6936 - acc: 0.457 - ETA: 1:59 - loss: 0.6934 - acc: 0.456 - ETA: 1:45 - loss: 0.6932 - acc: 0.485 - ETA: 1:34 - loss: 0.6930 - acc: 0.498 - ETA: 1:27 - loss: 0.6928 - acc: 0.502 - ETA: 1:20 - loss: 0.6927 - acc: 0.503 - ETA: 1:15 - loss: 0.6925 - acc: 0.508 - ETA: 1:11 - loss: 0.6924 - acc: 0.512 - ETA: 1:08 - loss: 0.6922 - acc: 0.518 - ETA: 1:05 - loss: 0.6920 - acc: 0.523 - ETA: 1:03 - loss: 0.6918 - acc: 0.532 - ETA: 1:02 - loss: 0.6918 - acc: 0.538 - ETA: 1:00 - loss: 0.6916 - acc: 0.545 - ETA: 58s - loss: 0.6914 - acc: 0.550 - ETA: 56s - loss: 0.6913 - acc: 0.55 - ETA: 55s - loss: 0.6911 - acc: 0.56 - ETA: 53s - loss: 0.6909 - acc: 0.57 - ETA: 52s - loss: 0.6907 - acc: 0.57 - ETA: 51s - loss: 0.6906 - acc: 0.57 - ETA: 50s - loss: 0.6905 - acc: 0.58 - ETA: 49s - loss: 0.6903 - acc: 0.58 - ETA: 48s - loss: 0.6902 - acc: 0.59 - ETA: 47s - loss: 0.6900 - acc: 0.59 - ETA: 47s - loss: 0.6898 - acc: 0.60 - ETA: 46s - loss: 0.6896 - acc: 0.60 - ETA: 46s - loss: 0.6894 - acc: 0.61 - ETA: 45s - loss: 0.6891 - acc: 0.61 - ETA: 44s - loss: 0.6889 - acc: 0.61 - ETA: 43s - loss: 0.6886 - acc: 0.62 - ETA: 43s - loss: 0.6883 - acc: 0.63 - ETA: 42s - loss: 0.6879 - acc: 0.63 - ETA: 42s - loss: 0.6877 - acc: 0.63 - ETA: 41s - loss: 0.6874 - acc: 0.63 - ETA: 40s - loss: 0.6870 - acc: 0.63 - ETA: 40s - loss: 0.6866 - acc: 0.64 - ETA: 39s - loss: 0.6864 - acc: 0.64 - ETA: 39s - loss: 0.6860 - acc: 0.64 - ETA: 38s - loss: 0.6855 - acc: 0.64 - ETA: 38s - loss: 0.6852 - acc: 0.65 - ETA: 38s - loss: 0.6847 - acc: 0.65 - ETA: 38s - loss: 0.6841 - acc: 0.65 - ETA: 37s - loss: 0.6835 - acc: 0.66 - ETA: 37s - loss: 0.6829 - acc: 0.66 - ETA: 36s - loss: 0.6823 - acc: 0.66 - ETA: 36s - loss: 0.6817 - acc: 0.66 - ETA: 36s - loss: 0.6807 - acc: 0.67 - ETA: 36s - loss: 0.6797 - acc: 0.67 - ETA: 35s - loss: 0.6785 - acc: 0.67 - ETA: 35s - loss: 0.6771 - acc: 0.68 - ETA: 35s - loss: 0.6756 - acc: 0.68 - ETA: 35s - loss: 0.6740 - acc: 0.68 - ETA: 34s - loss: 0.6716 - acc: 0.68 - ETA: 34s - loss: 0.6696 - acc: 0.68 - ETA: 34s - loss: 0.6668 - acc: 0.69 - ETA: 33s - loss: 0.6639 - acc: 0.69 - ETA: 33s - loss: 0.6604 - acc: 0.69 - ETA: 33s - loss: 0.6570 - acc: 0.69 - ETA: 32s - loss: 0.6538 - acc: 0.69 - ETA: 32s - loss: 0.6512 - acc: 0.70 - ETA: 32s - loss: 0.6469 - acc: 0.70 - ETA: 31s - loss: 0.6431 - acc: 0.70 - ETA: 31s - loss: 0.6403 - acc: 0.70 - ETA: 31s - loss: 0.6370 - acc: 0.70 - ETA: 30s - loss: 0.6332 - acc: 0.70 - ETA: 30s - loss: 0.6298 - acc: 0.71 - ETA: 30s - loss: 0.6277 - acc: 0.71 - ETA: 30s - loss: 0.6240 - acc: 0.71 - ETA: 29s - loss: 0.6217 - acc: 0.71 - ETA: 29s - loss: 0.6188 - acc: 0.71 - ETA: 29s - loss: 0.6170 - acc: 0.71 - ETA: 29s - loss: 0.6147 - acc: 0.71 - ETA: 29s - loss: 0.6130 - acc: 0.71 - ETA: 28s - loss: 0.6110 - acc: 0.71 - ETA: 28s - loss: 0.6090 - acc: 0.71 - ETA: 28s - loss: 0.6063 - acc: 0.72 - ETA: 28s - loss: 0.6051 - acc: 0.72 - ETA: 27s - loss: 0.6029 - acc: 0.72 - ETA: 27s - loss: 0.6001 - acc: 0.72 - ETA: 27s - loss: 0.5986 - acc: 0.72 - ETA: 27s - loss: 0.5963 - acc: 0.72 - ETA: 26s - loss: 0.5947 - acc: 0.72 - ETA: 26s - loss: 0.5928 - acc: 0.72 - ETA: 26s - loss: 0.5905 - acc: 0.73 - ETA: 25s - loss: 0.5887 - acc: 0.73 - ETA: 25s - loss: 0.5865 - acc: 0.73 - ETA: 25s - loss: 0.5852 - acc: 0.73 - ETA: 25s - loss: 0.5835 - acc: 0.73 - ETA: 24s - loss: 0.5817 - acc: 0.73 - ETA: 24s - loss: 0.5790 - acc: 0.73 - ETA: 24s - loss: 0.5775 - acc: 0.73 - ETA: 24s - loss: 0.5750 - acc: 0.73 - ETA: 24s - loss: 0.5725 - acc: 0.74 - ETA: 23s - loss: 0.5710 - acc: 0.74 - ETA: 23s - loss: 0.5690 - acc: 0.74 - ETA: 23s - loss: 0.5665 - acc: 0.74 - ETA: 23s - loss: 0.5642 - acc: 0.74 - ETA: 22s - loss: 0.5616 - acc: 0.74 - ETA: 22s - loss: 0.5589 - acc: 0.74 - ETA: 22s - loss: 0.5567 - acc: 0.74 - ETA: 22s - loss: 0.5543 - acc: 0.75 - ETA: 21s - loss: 0.5525 - acc: 0.75 - ETA: 21s - loss: 0.5513 - acc: 0.75 - ETA: 21s - loss: 0.5491 - acc: 0.75 - ETA: 20s - loss: 0.5470 - acc: 0.75 - ETA: 20s - loss: 0.5443 - acc: 0.75 - ETA: 20s - loss: 0.5436 - acc: 0.75 - ETA: 20s - loss: 0.5414 - acc: 0.75 - ETA: 19s - loss: 0.5401 - acc: 0.75 - ETA: 19s - loss: 0.5396 - acc: 0.75 - ETA: 19s - loss: 0.5377 - acc: 0.75 - ETA: 19s - loss: 0.5362 - acc: 0.75 - ETA: 19s - loss: 0.5351 - acc: 0.75 - ETA: 18s - loss: 0.5327 - acc: 0.76 - ETA: 18s - loss: 0.5319 - acc: 0.76 - ETA: 18s - loss: 0.5300 - acc: 0.76 - ETA: 18s - loss: 0.5282 - acc: 0.76 - ETA: 18s - loss: 0.5259 - acc: 0.76 - ETA: 17s - loss: 0.5238 - acc: 0.76 - ETA: 17s - loss: 0.5222 - acc: 0.76 - ETA: 17s - loss: 0.5211 - acc: 0.76 - ETA: 17s - loss: 0.5194 - acc: 0.76 - ETA: 16s - loss: 0.5181 - acc: 0.76 - ETA: 16s - loss: 0.5168 - acc: 0.76 - ETA: 16s - loss: 0.5150 - acc: 0.77 - ETA: 16s - loss: 0.5131 - acc: 0.77 - ETA: 16s - loss: 0.5111 - acc: 0.77 - ETA: 15s - loss: 0.5095 - acc: 0.77 - ETA: 15s - loss: 0.5078 - acc: 0.77 - ETA: 15s - loss: 0.5062 - acc: 0.77 - ETA: 15s - loss: 0.5048 - acc: 0.77 - ETA: 14s - loss: 0.5036 - acc: 0.77 - ETA: 14s - loss: 0.5020 - acc: 0.77 - ETA: 14s - loss: 0.5004 - acc: 0.77 - ETA: 14s - loss: 0.4993 - acc: 0.77 - ETA: 13s - loss: 0.4971 - acc: 0.77 - ETA: 13s - loss: 0.4962 - acc: 0.77 - ETA: 13s - loss: 0.4947 - acc: 0.78 - ETA: 13s - loss: 0.4939 - acc: 0.78 - ETA: 13s - loss: 0.4932 - acc: 0.78 - ETA: 12s - loss: 0.4921 - acc: 0.78 - ETA: 12s - loss: 0.4907 - acc: 0.78 - ETA: 12s - loss: 0.4891 - acc: 0.78 - ETA: 12s - loss: 0.4877 - acc: 0.78 - ETA: 11s - loss: 0.4864 - acc: 0.78 - ETA: 11s - loss: 0.4859 - acc: 0.78 - ETA: 11s - loss: 0.4850 - acc: 0.78 - ETA: 11s - loss: 0.4844 - acc: 0.78 - ETA: 11s - loss: 0.4830 - acc: 0.78 - ETA: 10s - loss: 0.4835 - acc: 0.78 - ETA: 10s - loss: 0.4827 - acc: 0.78 - ETA: 10s - loss: 0.4819 - acc: 0.78 - ETA: 10s - loss: 0.4809 - acc: 0.78 - ETA: 9s - loss: 0.4797 - acc: 0.7884 - ETA: 9s - loss: 0.4793 - acc: 0.788 - ETA: 9s - loss: 0.4787 - acc: 0.789 - ETA: 9s - loss: 0.4778 - acc: 0.789 - ETA: 9s - loss: 0.4769 - acc: 0.789 - ETA: 8s - loss: 0.4757 - acc: 0.790 - ETA: 8s - loss: 0.4746 - acc: 0.791 - ETA: 8s - loss: 0.4742 - acc: 0.791 - ETA: 8s - loss: 0.4729 - acc: 0.791 - ETA: 7s - loss: 0.4717 - acc: 0.792 - ETA: 7s - loss: 0.4711 - acc: 0.792 - ETA: 7s - loss: 0.4703 - acc: 0.792 - ETA: 7s - loss: 0.4695 - acc: 0.793 - ETA: 6s - loss: 0.4683 - acc: 0.793 - ETA: 6s - loss: 0.4679 - acc: 0.793 - ETA: 6s - loss: 0.4667 - acc: 0.794 - ETA: 6s - loss: 0.4653 - acc: 0.795 - ETA: 6s - loss: 0.4652 - acc: 0.795 - ETA: 5s - loss: 0.4645 - acc: 0.795 - ETA: 5s - loss: 0.4636 - acc: 0.795 - ETA: 5s - loss: 0.4621 - acc: 0.796 - ETA: 5s - loss: 0.4610 - acc: 0.797 - ETA: 4s - loss: 0.4605 - acc: 0.797 - ETA: 4s - loss: 0.4594 - acc: 0.797 - ETA: 4s - loss: 0.4593 - acc: 0.797 - ETA: 4s - loss: 0.4585 - acc: 0.798 - ETA: 4s - loss: 0.4576 - acc: 0.799 - ETA: 3s - loss: 0.4571 - acc: 0.799 - ETA: 3s - loss: 0.4565 - acc: 0.799 - ETA: 3s - loss: 0.4558 - acc: 0.799 - ETA: 3s - loss: 0.4548 - acc: 0.800 - ETA: 2s - loss: 0.4543 - acc: 0.800 - ETA: 2s - loss: 0.4533 - acc: 0.800 - ETA: 2s - loss: 0.4527 - acc: 0.801 - ETA: 2s - loss: 0.4520 - acc: 0.801 - ETA: 2s - loss: 0.4516 - acc: 0.801 - ETA: 1s - loss: 0.4510 - acc: 0.801 - ETA: 1s - loss: 0.4502 - acc: 0.802 - ETA: 1s - loss: 0.4497 - acc: 0.802 - ETA: 1s - loss: 0.4489 - acc: 0.802 - ETA: 0s - loss: 0.4484 - acc: 0.803 - ETA: 0s - loss: 0.4473 - acc: 0.803 - ETA: 0s - loss: 0.4464 - acc: 0.803 - ETA: 0s - loss: 0.4457 - acc: 0.804 - 47s 2ms/step - loss: 0.4456 - acc: 0.8043 - val_loss: 0.3403 - val_acc: 0.8548\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - ETA: 46s - loss: 0.2232 - acc: 0.93 - ETA: 48s - loss: 0.2261 - acc: 0.91 - ETA: 46s - loss: 0.2196 - acc: 0.92 - ETA: 57s - loss: 0.2268 - acc: 0.91 - ETA: 53s - loss: 0.2372 - acc: 0.90 - ETA: 50s - loss: 0.2277 - acc: 0.91 - ETA: 49s - loss: 0.2249 - acc: 0.91 - ETA: 51s - loss: 0.2373 - acc: 0.91 - ETA: 51s - loss: 0.2303 - acc: 0.91 - ETA: 50s - loss: 0.2280 - acc: 0.91 - ETA: 48s - loss: 0.2314 - acc: 0.91 - ETA: 47s - loss: 0.2287 - acc: 0.91 - ETA: 45s - loss: 0.2327 - acc: 0.91 - ETA: 44s - loss: 0.2334 - acc: 0.91 - ETA: 44s - loss: 0.2322 - acc: 0.91 - ETA: 43s - loss: 0.2371 - acc: 0.91 - ETA: 43s - loss: 0.2363 - acc: 0.90 - ETA: 42s - loss: 0.2312 - acc: 0.91 - ETA: 42s - loss: 0.2304 - acc: 0.91 - ETA: 41s - loss: 0.2309 - acc: 0.90 - ETA: 40s - loss: 0.2343 - acc: 0.90 - ETA: 40s - loss: 0.2296 - acc: 0.91 - ETA: 39s - loss: 0.2324 - acc: 0.91 - ETA: 39s - loss: 0.2317 - acc: 0.91 - ETA: 38s - loss: 0.2304 - acc: 0.91 - ETA: 38s - loss: 0.2338 - acc: 0.91 - ETA: 37s - loss: 0.2338 - acc: 0.91 - ETA: 37s - loss: 0.2367 - acc: 0.91 - ETA: 37s - loss: 0.2373 - acc: 0.90 - ETA: 37s - loss: 0.2381 - acc: 0.90 - ETA: 37s - loss: 0.2370 - acc: 0.90 - ETA: 36s - loss: 0.2360 - acc: 0.91 - ETA: 36s - loss: 0.2342 - acc: 0.91 - ETA: 35s - loss: 0.2328 - acc: 0.91 - ETA: 35s - loss: 0.2315 - acc: 0.91 - ETA: 35s - loss: 0.2316 - acc: 0.91 - ETA: 34s - loss: 0.2319 - acc: 0.91 - ETA: 34s - loss: 0.2310 - acc: 0.91 - ETA: 34s - loss: 0.2315 - acc: 0.91 - ETA: 34s - loss: 0.2311 - acc: 0.91 - ETA: 34s - loss: 0.2306 - acc: 0.91 - ETA: 34s - loss: 0.2301 - acc: 0.91 - ETA: 33s - loss: 0.2302 - acc: 0.91 - ETA: 33s - loss: 0.2296 - acc: 0.91 - ETA: 33s - loss: 0.2312 - acc: 0.91 - ETA: 32s - loss: 0.2310 - acc: 0.91 - ETA: 32s - loss: 0.2305 - acc: 0.91 - ETA: 32s - loss: 0.2301 - acc: 0.91 - ETA: 31s - loss: 0.2298 - acc: 0.91 - ETA: 31s - loss: 0.2291 - acc: 0.91 - ETA: 31s - loss: 0.2302 - acc: 0.91 - ETA: 30s - loss: 0.2308 - acc: 0.91 - ETA: 30s - loss: 0.2310 - acc: 0.91 - ETA: 30s - loss: 0.2325 - acc: 0.90 - ETA: 29s - loss: 0.2327 - acc: 0.90 - ETA: 29s - loss: 0.2340 - acc: 0.90 - ETA: 29s - loss: 0.2343 - acc: 0.90 - ETA: 29s - loss: 0.2355 - acc: 0.90 - ETA: 29s - loss: 0.2368 - acc: 0.90 - ETA: 28s - loss: 0.2366 - acc: 0.90 - ETA: 28s - loss: 0.2370 - acc: 0.90 - ETA: 28s - loss: 0.2368 - acc: 0.90 - ETA: 27s - loss: 0.2354 - acc: 0.90 - ETA: 27s - loss: 0.2362 - acc: 0.90 - ETA: 27s - loss: 0.2368 - acc: 0.90 - ETA: 27s - loss: 0.2353 - acc: 0.90 - ETA: 26s - loss: 0.2366 - acc: 0.90 - ETA: 26s - loss: 0.2370 - acc: 0.90 - ETA: 26s - loss: 0.2366 - acc: 0.90 - ETA: 25s - loss: 0.2358 - acc: 0.90 - ETA: 25s - loss: 0.2360 - acc: 0.90 - ETA: 25s - loss: 0.2384 - acc: 0.90 - ETA: 25s - loss: 0.2380 - acc: 0.90 - ETA: 24s - loss: 0.2371 - acc: 0.90 - ETA: 24s - loss: 0.2382 - acc: 0.90 - ETA: 24s - loss: 0.2373 - acc: 0.90 - ETA: 24s - loss: 0.2370 - acc: 0.90 - ETA: 23s - loss: 0.2376 - acc: 0.90 - ETA: 23s - loss: 0.2383 - acc: 0.90 - ETA: 23s - loss: 0.2390 - acc: 0.90 - ETA: 23s - loss: 0.2387 - acc: 0.90 - ETA: 23s - loss: 0.2383 - acc: 0.90 - ETA: 23s - loss: 0.2381 - acc: 0.90 - ETA: 23s - loss: 0.2380 - acc: 0.90 - ETA: 23s - loss: 0.2379 - acc: 0.90 - ETA: 23s - loss: 0.2380 - acc: 0.90 - ETA: 23s - loss: 0.2380 - acc: 0.90 - ETA: 22s - loss: 0.2373 - acc: 0.90 - ETA: 22s - loss: 0.2372 - acc: 0.90 - ETA: 22s - loss: 0.2363 - acc: 0.90 - ETA: 22s - loss: 0.2367 - acc: 0.90 - ETA: 22s - loss: 0.2369 - acc: 0.90 - ETA: 22s - loss: 0.2369 - acc: 0.90 - ETA: 22s - loss: 0.2367 - acc: 0.90 - ETA: 22s - loss: 0.2366 - acc: 0.90 - ETA: 21s - loss: 0.2362 - acc: 0.90 - ETA: 21s - loss: 0.2362 - acc: 0.90 - ETA: 21s - loss: 0.2363 - acc: 0.90 - ETA: 21s - loss: 0.2360 - acc: 0.90 - ETA: 20s - loss: 0.2366 - acc: 0.90 - ETA: 20s - loss: 0.2374 - acc: 0.90 - ETA: 20s - loss: 0.2384 - acc: 0.90 - ETA: 20s - loss: 0.2379 - acc: 0.90 - ETA: 20s - loss: 0.2378 - acc: 0.90 - ETA: 20s - loss: 0.2386 - acc: 0.90 - ETA: 19s - loss: 0.2383 - acc: 0.90 - ETA: 19s - loss: 0.2384 - acc: 0.90 - ETA: 19s - loss: 0.2376 - acc: 0.90 - ETA: 19s - loss: 0.2377 - acc: 0.90 - ETA: 19s - loss: 0.2380 - acc: 0.90 - ETA: 18s - loss: 0.2377 - acc: 0.90 - ETA: 18s - loss: 0.2375 - acc: 0.90 - ETA: 18s - loss: 0.2378 - acc: 0.90 - ETA: 18s - loss: 0.2378 - acc: 0.90 - ETA: 17s - loss: 0.2378 - acc: 0.90 - ETA: 17s - loss: 0.2375 - acc: 0.90 - ETA: 17s - loss: 0.2372 - acc: 0.90 - ETA: 17s - loss: 0.2370 - acc: 0.90 - ETA: 17s - loss: 0.2368 - acc: 0.90 - ETA: 16s - loss: 0.2377 - acc: 0.90 - ETA: 16s - loss: 0.2378 - acc: 0.90 - ETA: 16s - loss: 0.2378 - acc: 0.90 - ETA: 16s - loss: 0.2376 - acc: 0.90 - ETA: 15s - loss: 0.2378 - acc: 0.90 - ETA: 15s - loss: 0.2374 - acc: 0.90 - ETA: 15s - loss: 0.2371 - acc: 0.90 - ETA: 15s - loss: 0.2367 - acc: 0.90 - ETA: 15s - loss: 0.2366 - acc: 0.90 - ETA: 14s - loss: 0.2367 - acc: 0.90 - ETA: 14s - loss: 0.2365 - acc: 0.90 - ETA: 14s - loss: 0.2361 - acc: 0.90 - ETA: 14s - loss: 0.2366 - acc: 0.90 - ETA: 13s - loss: 0.2369 - acc: 0.90 - ETA: 13s - loss: 0.2364 - acc: 0.90 - ETA: 13s - loss: 0.2359 - acc: 0.90 - ETA: 13s - loss: 0.2356 - acc: 0.90 - ETA: 13s - loss: 0.2365 - acc: 0.90 - ETA: 12s - loss: 0.2368 - acc: 0.90 - ETA: 12s - loss: 0.2365 - acc: 0.90 - ETA: 12s - loss: 0.2358 - acc: 0.90 - ETA: 12s - loss: 0.2360 - acc: 0.90 - ETA: 12s - loss: 0.2366 - acc: 0.90 - ETA: 11s - loss: 0.2369 - acc: 0.90 - ETA: 11s - loss: 0.2367 - acc: 0.90 - ETA: 11s - loss: 0.2369 - acc: 0.90 - ETA: 11s - loss: 0.2374 - acc: 0.90 - ETA: 11s - loss: 0.2368 - acc: 0.90 - ETA: 10s - loss: 0.2372 - acc: 0.90 - ETA: 10s - loss: 0.2370 - acc: 0.90 - ETA: 10s - loss: 0.2373 - acc: 0.90 - ETA: 10s - loss: 0.2373 - acc: 0.90 - ETA: 9s - loss: 0.2369 - acc: 0.9080 - ETA: 9s - loss: 0.2375 - acc: 0.907 - ETA: 9s - loss: 0.2378 - acc: 0.907 - ETA: 9s - loss: 0.2375 - acc: 0.908 - ETA: 9s - loss: 0.2370 - acc: 0.908 - ETA: 8s - loss: 0.2372 - acc: 0.908 - ETA: 8s - loss: 0.2377 - acc: 0.907 - ETA: 8s - loss: 0.2384 - acc: 0.907 - ETA: 8s - loss: 0.2382 - acc: 0.907 - ETA: 8s - loss: 0.2385 - acc: 0.907 - ETA: 7s - loss: 0.2385 - acc: 0.907 - ETA: 7s - loss: 0.2380 - acc: 0.907 - ETA: 7s - loss: 0.2380 - acc: 0.907 - ETA: 7s - loss: 0.2376 - acc: 0.907 - ETA: 7s - loss: 0.2380 - acc: 0.907 - ETA: 6s - loss: 0.2380 - acc: 0.907 - ETA: 6s - loss: 0.2379 - acc: 0.907 - ETA: 6s - loss: 0.2378 - acc: 0.907 - ETA: 6s - loss: 0.2380 - acc: 0.907 - ETA: 5s - loss: 0.2380 - acc: 0.907 - ETA: 5s - loss: 0.2381 - acc: 0.907 - ETA: 5s - loss: 0.2377 - acc: 0.907 - ETA: 5s - loss: 0.2374 - acc: 0.907 - ETA: 5s - loss: 0.2375 - acc: 0.907 - ETA: 4s - loss: 0.2375 - acc: 0.907 - ETA: 4s - loss: 0.2373 - acc: 0.907 - ETA: 4s - loss: 0.2377 - acc: 0.907 - ETA: 4s - loss: 0.2380 - acc: 0.907 - ETA: 4s - loss: 0.2387 - acc: 0.907 - ETA: 3s - loss: 0.2387 - acc: 0.907 - ETA: 3s - loss: 0.2387 - acc: 0.907 - ETA: 3s - loss: 0.2384 - acc: 0.907 - ETA: 3s - loss: 0.2385 - acc: 0.907 - ETA: 3s - loss: 0.2380 - acc: 0.907 - ETA: 2s - loss: 0.2375 - acc: 0.907 - ETA: 2s - loss: 0.2373 - acc: 0.907 - ETA: 2s - loss: 0.2370 - acc: 0.907 - ETA: 2s - loss: 0.2367 - acc: 0.907 - ETA: 2s - loss: 0.2370 - acc: 0.907 - ETA: 1s - loss: 0.2366 - acc: 0.907 - ETA: 1s - loss: 0.2372 - acc: 0.907 - ETA: 1s - loss: 0.2371 - acc: 0.907 - ETA: 1s - loss: 0.2374 - acc: 0.907 - ETA: 1s - loss: 0.2375 - acc: 0.907 - ETA: 0s - loss: 0.2377 - acc: 0.907 - ETA: 0s - loss: 0.2379 - acc: 0.907 - ETA: 0s - loss: 0.2381 - acc: 0.907 - ETA: 0s - loss: 0.2382 - acc: 0.907 - 43s 2ms/step - loss: 0.2384 - acc: 0.9073 - val_loss: 0.2993 - val_acc: 0.8740\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - ETA: 48s - loss: 0.1137 - acc: 0.98 - ETA: 46s - loss: 0.1899 - acc: 0.93 - ETA: 46s - loss: 0.1800 - acc: 0.94 - ETA: 45s - loss: 0.1756 - acc: 0.94 - ETA: 45s - loss: 0.1808 - acc: 0.94 - ETA: 50s - loss: 0.1734 - acc: 0.95 - ETA: 50s - loss: 0.1658 - acc: 0.95 - ETA: 49s - loss: 0.1743 - acc: 0.94 - ETA: 49s - loss: 0.1766 - acc: 0.94 - ETA: 48s - loss: 0.1736 - acc: 0.94 - ETA: 47s - loss: 0.1779 - acc: 0.94 - ETA: 46s - loss: 0.1747 - acc: 0.94 - ETA: 46s - loss: 0.1715 - acc: 0.94 - ETA: 45s - loss: 0.1672 - acc: 0.94 - ETA: 44s - loss: 0.1673 - acc: 0.94 - ETA: 44s - loss: 0.1678 - acc: 0.94 - ETA: 43s - loss: 0.1680 - acc: 0.94 - ETA: 43s - loss: 0.1650 - acc: 0.94 - ETA: 42s - loss: 0.1622 - acc: 0.94 - ETA: 42s - loss: 0.1636 - acc: 0.94 - ETA: 42s - loss: 0.1701 - acc: 0.94 - ETA: 41s - loss: 0.1691 - acc: 0.94 - ETA: 41s - loss: 0.1667 - acc: 0.94 - ETA: 41s - loss: 0.1633 - acc: 0.94 - ETA: 40s - loss: 0.1601 - acc: 0.94 - ETA: 40s - loss: 0.1561 - acc: 0.95 - ETA: 40s - loss: 0.1577 - acc: 0.94 - ETA: 39s - loss: 0.1556 - acc: 0.94 - ETA: 39s - loss: 0.1553 - acc: 0.94 - ETA: 40s - loss: 0.1546 - acc: 0.94 - ETA: 40s - loss: 0.1532 - acc: 0.94 - ETA: 39s - loss: 0.1518 - acc: 0.95 - ETA: 39s - loss: 0.1528 - acc: 0.94 - ETA: 39s - loss: 0.1530 - acc: 0.94 - ETA: 38s - loss: 0.1521 - acc: 0.94 - ETA: 37s - loss: 0.1522 - acc: 0.94 - ETA: 37s - loss: 0.1535 - acc: 0.94 - ETA: 36s - loss: 0.1542 - acc: 0.94 - ETA: 36s - loss: 0.1535 - acc: 0.94 - ETA: 36s - loss: 0.1530 - acc: 0.94 - ETA: 35s - loss: 0.1516 - acc: 0.94 - ETA: 35s - loss: 0.1534 - acc: 0.94 - ETA: 34s - loss: 0.1522 - acc: 0.94 - ETA: 34s - loss: 0.1546 - acc: 0.94 - ETA: 33s - loss: 0.1537 - acc: 0.94 - ETA: 33s - loss: 0.1540 - acc: 0.94 - ETA: 33s - loss: 0.1541 - acc: 0.94 - ETA: 32s - loss: 0.1539 - acc: 0.94 - ETA: 32s - loss: 0.1553 - acc: 0.94 - ETA: 32s - loss: 0.1548 - acc: 0.94 - ETA: 31s - loss: 0.1557 - acc: 0.94 - ETA: 31s - loss: 0.1555 - acc: 0.94 - ETA: 31s - loss: 0.1560 - acc: 0.94 - ETA: 30s - loss: 0.1557 - acc: 0.94 - ETA: 30s - loss: 0.1562 - acc: 0.94 - ETA: 30s - loss: 0.1561 - acc: 0.94 - ETA: 29s - loss: 0.1571 - acc: 0.94 - ETA: 29s - loss: 0.1565 - acc: 0.94 - ETA: 29s - loss: 0.1554 - acc: 0.94 - ETA: 29s - loss: 0.1555 - acc: 0.94 - ETA: 29s - loss: 0.1570 - acc: 0.94 - ETA: 28s - loss: 0.1566 - acc: 0.94 - ETA: 28s - loss: 0.1573 - acc: 0.94 - ETA: 28s - loss: 0.1567 - acc: 0.94 - ETA: 27s - loss: 0.1565 - acc: 0.94 - ETA: 27s - loss: 0.1564 - acc: 0.94 - ETA: 27s - loss: 0.1551 - acc: 0.94 - ETA: 27s - loss: 0.1548 - acc: 0.94 - ETA: 26s - loss: 0.1556 - acc: 0.94 - ETA: 26s - loss: 0.1546 - acc: 0.94 - ETA: 26s - loss: 0.1548 - acc: 0.94 - ETA: 26s - loss: 0.1549 - acc: 0.94 - ETA: 25s - loss: 0.1550 - acc: 0.94 - ETA: 25s - loss: 0.1556 - acc: 0.94 - ETA: 25s - loss: 0.1557 - acc: 0.94 - ETA: 25s - loss: 0.1560 - acc: 0.94 - ETA: 24s - loss: 0.1550 - acc: 0.94 - ETA: 24s - loss: 0.1551 - acc: 0.94 - ETA: 24s - loss: 0.1548 - acc: 0.94 - ETA: 24s - loss: 0.1543 - acc: 0.94 - ETA: 23s - loss: 0.1541 - acc: 0.94 - ETA: 23s - loss: 0.1546 - acc: 0.94 - ETA: 23s - loss: 0.1543 - acc: 0.94 - ETA: 23s - loss: 0.1536 - acc: 0.94 - ETA: 22s - loss: 0.1541 - acc: 0.94 - ETA: 22s - loss: 0.1545 - acc: 0.94 - ETA: 22s - loss: 0.1553 - acc: 0.94 - ETA: 22s - loss: 0.1543 - acc: 0.94 - ETA: 22s - loss: 0.1540 - acc: 0.94 - ETA: 22s - loss: 0.1538 - acc: 0.94 - ETA: 22s - loss: 0.1538 - acc: 0.94 - ETA: 21s - loss: 0.1540 - acc: 0.94 - ETA: 21s - loss: 0.1551 - acc: 0.94 - ETA: 21s - loss: 0.1551 - acc: 0.94 - ETA: 21s - loss: 0.1553 - acc: 0.94 - ETA: 20s - loss: 0.1550 - acc: 0.94 - ETA: 20s - loss: 0.1547 - acc: 0.94 - ETA: 20s - loss: 0.1546 - acc: 0.94 - ETA: 20s - loss: 0.1548 - acc: 0.94 - ETA: 20s - loss: 0.1567 - acc: 0.94 - ETA: 19s - loss: 0.1575 - acc: 0.94 - ETA: 19s - loss: 0.1572 - acc: 0.94 - ETA: 19s - loss: 0.1571 - acc: 0.94 - ETA: 19s - loss: 0.1572 - acc: 0.94 - ETA: 18s - loss: 0.1576 - acc: 0.94 - ETA: 18s - loss: 0.1579 - acc: 0.94 - ETA: 18s - loss: 0.1586 - acc: 0.94 - ETA: 18s - loss: 0.1581 - acc: 0.94 - ETA: 18s - loss: 0.1580 - acc: 0.94 - ETA: 17s - loss: 0.1584 - acc: 0.94 - ETA: 17s - loss: 0.1583 - acc: 0.94 - ETA: 17s - loss: 0.1585 - acc: 0.94 - ETA: 17s - loss: 0.1598 - acc: 0.94 - ETA: 17s - loss: 0.1605 - acc: 0.94 - ETA: 16s - loss: 0.1604 - acc: 0.94 - ETA: 16s - loss: 0.1608 - acc: 0.94 - ETA: 16s - loss: 0.1608 - acc: 0.94 - ETA: 16s - loss: 0.1618 - acc: 0.94 - ETA: 16s - loss: 0.1620 - acc: 0.94 - ETA: 16s - loss: 0.1620 - acc: 0.94 - ETA: 15s - loss: 0.1622 - acc: 0.94 - ETA: 15s - loss: 0.1624 - acc: 0.94 - ETA: 15s - loss: 0.1622 - acc: 0.94 - ETA: 15s - loss: 0.1620 - acc: 0.94 - ETA: 14s - loss: 0.1629 - acc: 0.94 - ETA: 14s - loss: 0.1626 - acc: 0.94 - ETA: 14s - loss: 0.1639 - acc: 0.94 - ETA: 14s - loss: 0.1645 - acc: 0.94 - ETA: 14s - loss: 0.1642 - acc: 0.94 - ETA: 13s - loss: 0.1638 - acc: 0.94 - ETA: 13s - loss: 0.1641 - acc: 0.94 - ETA: 13s - loss: 0.1642 - acc: 0.93 - ETA: 13s - loss: 0.1642 - acc: 0.94 - ETA: 13s - loss: 0.1644 - acc: 0.93 - ETA: 12s - loss: 0.1642 - acc: 0.93 - ETA: 12s - loss: 0.1647 - acc: 0.93 - ETA: 12s - loss: 0.1654 - acc: 0.93 - ETA: 12s - loss: 0.1652 - acc: 0.93 - ETA: 12s - loss: 0.1656 - acc: 0.93 - ETA: 11s - loss: 0.1666 - acc: 0.93 - ETA: 11s - loss: 0.1673 - acc: 0.93 - ETA: 11s - loss: 0.1678 - acc: 0.93 - ETA: 11s - loss: 0.1687 - acc: 0.93 - ETA: 11s - loss: 0.1689 - acc: 0.93 - ETA: 11s - loss: 0.1688 - acc: 0.93 - ETA: 10s - loss: 0.1686 - acc: 0.93 - ETA: 10s - loss: 0.1684 - acc: 0.93 - ETA: 10s - loss: 0.1687 - acc: 0.93 - ETA: 10s - loss: 0.1692 - acc: 0.93 - ETA: 10s - loss: 0.1693 - acc: 0.93 - ETA: 9s - loss: 0.1693 - acc: 0.9377 - ETA: 9s - loss: 0.1691 - acc: 0.937 - ETA: 9s - loss: 0.1688 - acc: 0.937 - ETA: 9s - loss: 0.1690 - acc: 0.937 - ETA: 9s - loss: 0.1699 - acc: 0.937 - ETA: 8s - loss: 0.1703 - acc: 0.937 - ETA: 8s - loss: 0.1708 - acc: 0.937 - ETA: 8s - loss: 0.1706 - acc: 0.937 - ETA: 8s - loss: 0.1712 - acc: 0.937 - ETA: 8s - loss: 0.1715 - acc: 0.936 - ETA: 7s - loss: 0.1719 - acc: 0.936 - ETA: 7s - loss: 0.1722 - acc: 0.936 - ETA: 7s - loss: 0.1721 - acc: 0.936 - ETA: 7s - loss: 0.1723 - acc: 0.936 - ETA: 7s - loss: 0.1722 - acc: 0.936 - ETA: 6s - loss: 0.1718 - acc: 0.936 - ETA: 6s - loss: 0.1718 - acc: 0.936 - ETA: 6s - loss: 0.1725 - acc: 0.936 - ETA: 6s - loss: 0.1727 - acc: 0.936 - ETA: 6s - loss: 0.1724 - acc: 0.936 - ETA: 5s - loss: 0.1724 - acc: 0.936 - ETA: 5s - loss: 0.1723 - acc: 0.936 - ETA: 5s - loss: 0.1724 - acc: 0.936 - ETA: 5s - loss: 0.1718 - acc: 0.936 - ETA: 5s - loss: 0.1719 - acc: 0.936 - ETA: 4s - loss: 0.1722 - acc: 0.936 - ETA: 4s - loss: 0.1722 - acc: 0.936 - ETA: 4s - loss: 0.1722 - acc: 0.937 - ETA: 4s - loss: 0.1734 - acc: 0.936 - ETA: 4s - loss: 0.1734 - acc: 0.936 - ETA: 3s - loss: 0.1732 - acc: 0.936 - ETA: 3s - loss: 0.1733 - acc: 0.936 - ETA: 3s - loss: 0.1737 - acc: 0.936 - ETA: 3s - loss: 0.1740 - acc: 0.936 - ETA: 3s - loss: 0.1741 - acc: 0.936 - ETA: 2s - loss: 0.1743 - acc: 0.936 - ETA: 2s - loss: 0.1743 - acc: 0.936 - ETA: 2s - loss: 0.1739 - acc: 0.936 - ETA: 2s - loss: 0.1743 - acc: 0.936 - ETA: 2s - loss: 0.1742 - acc: 0.936 - ETA: 1s - loss: 0.1742 - acc: 0.936 - ETA: 1s - loss: 0.1742 - acc: 0.936 - ETA: 1s - loss: 0.1741 - acc: 0.936 - ETA: 1s - loss: 0.1740 - acc: 0.936 - ETA: 1s - loss: 0.1739 - acc: 0.936 - ETA: 0s - loss: 0.1738 - acc: 0.936 - ETA: 0s - loss: 0.1735 - acc: 0.936 - ETA: 0s - loss: 0.1737 - acc: 0.936 - ETA: 0s - loss: 0.1737 - acc: 0.936 - 44s 2ms/step - loss: 0.1742 - acc: 0.9366 - val_loss: 0.3381 - val_acc: 0.8682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1270cce5390>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_size))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 3\n",
    "model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"naive_bayes.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"naive_bayes.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         768000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 64)          41216     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                1300      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 810,537\n",
      "Trainable params: 810,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>Naturally in a film who's main themes are of m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             review\n",
       "0  12311_10  Naturally in a film who's main themes are of m..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = \"D:/DataRepository/final_project/Final-Project-IMDB-Reviews/testData.tsv\"\n",
    "\n",
    "# Import the CSV into a pandas DataFrame\n",
    "df_test = pd.read_csv(csv_path, low_memory=False, sep = '\\t')\n",
    "\n",
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"review\"]=df_test.review.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"sentiment\"] = df_test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"review\"]=df_test.review.apply(lambda x: clean_text(x))\n",
    "# df_test[\"sentiment\"] = df_test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\n",
    "y_test = df_test[\"sentiment\"]\n",
    "list_sentences_test = df_test[\"review\"]\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "prediction = model.predict(X_te)\n",
    "y_pred = (prediction > 0.5)\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "print('F1-score: {0}'.format(f1_score(y_pred, y_test)))\n",
    "print('Confusion matrix:')\n",
    "confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"id\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt='d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['good review']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=[[\"good review\"]]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([\"good review\",\"bad review\",\"excellent\"])\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "test=test.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['good review'],\n",
       "       ['bad review'],\n",
       "       ['excellent']], dtype='<U11')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-38c4bc3cdaac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "for column in test.reshape(-1,1):\n",
    "    if test[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        test[column] = le.fit_transform(test[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'good review'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-3e3eea83b9bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_scaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_train_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_test_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    349\u001b[0m         X = check_array(X, copy=self.copy, warn_on_dtype=True,\n\u001b[0;32m    350\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m                         force_all_finite=\"allow-nan\")\n\u001b[0m\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[0mdata_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \"\"\"\n\u001b[1;32m--> 501\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'good review'"
     ]
    }
   ],
   "source": [
    "X_scaler = MinMaxScaler().fit(test)\n",
    "\n",
    "\n",
    "X_train_scaled = X_scaler.transform(test)\n",
    "X_test_scaled = X_scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: [[0]]\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Predicted class: {model.predict_classes(test)}\")\n",
    "print(f\"Predicted class: {model.predict_classes(test.shape[:100])}\")\n",
    "\n",
    "# X_test_scaled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
